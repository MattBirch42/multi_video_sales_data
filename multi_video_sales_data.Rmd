---
title: "Some Fundamentals"
author: "Matt Birch"
date: "2024-05-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

# Overview

This project uses an artificial data set to give users practice in many areas that are relevant to data science practitioners. This document will have sections for each of the relevant videos, and subsections within those to divide concepts. 

# Exploring a New Data Set

Some of the most important work you will ever do as a data person is in this section. You have to have good data to get good results. In this section, I will highlight some common processes related to Exploratory Data Analysis (EDA) and data cleaning. 

## Video Link: 

## First glimpses of data

The first thing I am going to do when I get a new data set is look at data structures. I am a big fan of the `head()` function which tells you variable classes and presents the first few values. 

```{r}

# read in data

url <- "https://raw.githubusercontent.com/MattBirch42/multi_video_sales_data/main/project_data.csv"

myData <- read.csv(url)

head(myData)

```
Right away, I can see that most features seem to be stored correctly. There are two exceptions:

* Volume: Those numbers are stored as characters and have commas in them. Let's fix that before we move on. 

* Date: currently stored as character. It appears to be in ymd format, but I usually like to check these more before just going for it. 

Let's also check for missing values for each variable:

```{r}
for(var in names(myData)) {
  nas <- sum(is.na(myData[,var]))
  if(nas != 0) {
    print(paste0("Missing ",var," values: ",nas))
  }
  
}
```

## Fixing Volume

We'll start with volume. I will substitute commas for nothing, and then I will convert to numeric.

```{r}
myData$volume <- as.numeric(gsub(",","",myData$volume))

str(myData$volume)

```

## Fixing Dates

Now that that is fixed, lets get a better look at the date feature. One way to check dates is to graph a variable over time. If there are holes in your date feature, they will often show up in the graph. Let's format date as a ymd date variable and then calculate sum volume over time. 


```{r}

myData$date2 <- as.Date(myData$date)

date_counts <- myData%>%
  group_by(date2) %>%
  summarise(count = n())

ggplot(date_counts, aes(x = date2, y = count)) +
  geom_line() +  
  labs(x = "Date", y = "Number of Rows", title = "Time Series Plot of Row Counts by Date") + ylim(0,100)
```

Woah Nellie! Those are years on teh x axis, and something is super wrong with our date variable! The first few rows looked good, but this graph is terrible! It is very possibly an issue with inconsistent formatting. 

Let's start by isolating cases where the raw date does not match this one `myData$date2 <- as.Date(myData$date)`:

```{r}
dateTemp <- myData %>%
  select(date,date2) %>%
  mutate(date = as.character(date),
         date2 = as.character(date2)) %>%
  distinct()

dateTemp <- subset(dateTemp, date != date2, na.rm = T)

dateTemp
```

This shows us where some encodings are wrong. In this case, the 2022 data was in mdy form instead of ymd. So converting the whole string to ymd was wrong. There are multiple ways to clean this. The structure inconsistency is simple enough here that I think this way will be easiest:

```{r}
rowsToModify <- grep("-2022",myData$date)

myData$date[rowsToModify] <- gsub("-2022","",myData$date[rowsToModify])

myData$date[rowsToModify] <- paste0("2022-",myData$date[rowsToModify])

```

Let's also just check the unique year values of the date variable after this change.

```{r}
sort(unique(year(myData$date)))
```
That is weird. If you dig in a little further, you will see that all of the 2020 dates were messed up and recorded as the year 0020. It might not always be so easy to fix messy dates. They can get messed up in so many ways. 

```{r}
myData$date <- gsub("0020","2020",myData$date)

myData$date <- ymd(myData$date)

```

It is hard to know what is going on with our dates, but maybe we should just check how many rows of data are on each date. I would like to see some consistency or pattern, without major holes or absurd spikes. (and, going back to the first graph, no dates from the 1st century). And I see consistently filled dates with some sort of seasonal pattern. I am going to leave dates alone for now. 

```{r}
date_counts <- myData%>%
  group_by(date) %>%
  summarise(count = n())

ggplot(date_counts, aes(x = date, y = count)) +
  geom_line() +  
  labs(x = "Date", y = "Number of Rows", title = "Time Series Plot of Row Counts by Date")
```


## Other categorical variables

Let's check the other categorical variables, too. This data set is small enough, we can take a simple approach and check unique values. In my experience, you will often find annoying little errors in these.

### Brand

We need some domain knowledge here. For this project, know that there are 9 brands, labeled brand01, brand02, ... brand09. Our data set, however, has `r length(unique(myData$brand))` unique values in the brand column. Let's take a look.

```{r}
sort(unique(myData$brand))

```

Well that is annoying! Some are misspelled. Some have weird little prefixes. Some are lower case and some are upper case. Some have spaces and some don't. Imagine a bunch of eople in different offices through the company, all writing the same thing in different ways. It happens!

```{r}
myData$brand <- tolower(myData$brand) #everything to lower case

# remove patterns we do not like and replace them with patters we do like. 
myData$brand <- gsub("br code: ", "", myData$brand)

myData$brand <- gsub(" ", "", myData$brand)

myData$brand <- gsub("brnd","brand",myData$brand)

myData$brand <- gsub("#","",myData$brand)

sort(unique(myData$brand))

```

### Segment

Market segments look fine. The data is made up, so we don't know what A, B, or C mean, but the are consistently labeled. 

```{r}
# Check for inconsistent formatting
unique_segments <- unique(myData$segment)

print(sort(unique_segments))
```

### zone

```{r}
unique_zones <- unique(myData$zone)

print(unique_zones)

```
```{r}
myData$zone <- tolower(myData$zone)

unique(myData$zone)
```
### region

```{r}
unique_regions <- unique(myData$region)

print(unique_regions)
```
Another mess!

```{r}
myData$region <- gsub(" ","",myData$region)
myData$region <- gsub("#","",myData$region)
myData$region <- toupper(myData$region)
myData$region <- gsub("REGOIN","REGION",myData$region)
myData$region <- gsub("RREGION","REGION",myData$region)
myData$region <- tolower(myData$region)

sort(unique(myData$region))
```
### customerID

```{r}
unique_customerID <- unique(myData$customerID)

print(unique_customerID)

```
I don't want to accidentally treat these like numbers in the future, so I am going to opt to keep "CID_" on them. I want it on all of them. This is not required. As long as R thins it is a character, it is fine. This is just a preference to help with recognizing the data. 

This code tests whether the customerID begins with "CID_", and if it doesn't, it adds it. 


```{r}
myData$customerID2 <- ifelse(grepl("^CID_", myData$customerID), myData$customerID, paste0("CID_", myData$customerID))
```

Don't forget, though, that we had `r (sum(myData$customerID == "CID_NA"))` missing values, that are stored as "CID_NA" now. 

In practice, you will need context to know what to do with these. It is not numeric, so it cannot be imputed. You could leave the CID_NA tag as a catch-all bin for unlabeled customers. Or you could drop those values from the data set. Or you could try to do something fancy. We'll just leave this as is. CID_NA will be the value we use to describe unidentified customers.  


```{r}

# Summary statistics
summary(myData)

```


```{r}

# Count missing values per variable
missing_values <- colSums(is.na(myData))
missing_values

print(paste0("There are ",nrow(myData), " rows in the data. This is how many NA values exist by variable:"))
# Display only columns with missing values
missing_values[missing_values > 0]

```


```{r}
# Visualize potential outliers
ggplot(myData, aes(x = listPrice)) +
  geom_histogram() +
  ggtitle("Distribution of List Price")

ggplot(myData, aes(x = salesPrice)) +
  geom_histogram() +
  ggtitle("Distribution of Sales Price")

ggplot(myData, aes(x = volume)) +
  geom_histogram() +
  ggtitle("Distribution of Volume")
```



```{r}






# Check for duplicates
duplicate_rows <- myData %>%
  count(brand, zone, region, customerID, listPrice, discount, salesPrice, volume, date) %>%
  filter(n > 1)

print(duplicate_rows)



# Histograms for numerical variables
myData %>%
  select(where(is.numeric)) %>%  # Select numeric columns
  pivot_longer(cols = everything()) %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 10) +
  facet_wrap(~name, scales = "free")

# Bar charts for categorical variables
myData %>%
  select(where(is.character)) %>%  # Select character columns
  pivot_longer(cols = everything()) %>%
  ggplot(aes(x = value)) +
  geom_bar() +
  facet_wrap(~name, scales = "free") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels
